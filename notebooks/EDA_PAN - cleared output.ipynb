{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZOcnWG89s_Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1\n",
        "## Цель задачи - бинарная классификация данных (сгенерированные с помощь AI или нет)"
      ],
      "metadata": {
        "id": "Fm_j2b8N-akz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Загрузка данных"
      ],
      "metadata": {
        "id": "AEv3yMohdRBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1lGIpg3OhOAlNPgBDTAv5AYgpIdEz2Isa # data\n",
        "!gdown 1_R7mVJMgVxdlC5-TjdLnxB8HTX9unZ-_ # task"
      ],
      "metadata": {
        "id": "3i8N7QzC-ZUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 14962653.zip"
      ],
      "metadata": {
        "id": "pAtBD02PeBv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip pan25-generative-ai-detection-task1-train.zip"
      ],
      "metadata": {
        "id": "1w_OHADAeVv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_json(\"train.jsonl\", lines=True)\n",
        "val_df = pd.read_json(\"val.jsonl\", lines=True)"
      ],
      "metadata": {
        "id": "cyzR2czVedJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Размер валидации: {val_df.shape[0]}\")\n",
        "val_df.head()"
      ],
      "metadata": {
        "id": "YyuDXp8BeoUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Размер трейна: {train_df.shape[0]}\")\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "7CcLuQbBerTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df[\"split\"] = \"val\"\n",
        "train_df[\"split\"] = \"train\"\n",
        "df = pd.concat([train_df, val_df])"
      ],
      "metadata": {
        "id": "4L5gF7BCe5qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "7A8tOrwUfIDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum() # Данные без пропусков"
      ],
      "metadata": {
        "id": "aqRrlFv-fKng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "посмотрим на распределение по моделям, датасетам и таргетам"
      ],
      "metadata": {
        "id": "-p_dh_lXfNph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text_len\"] = df[\"text\"].str.len()"
      ],
      "metadata": {
        "id": "7af6PRSGfToV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_plot = ['model', 'genre', 'label']\n",
        "colors = {\n",
        "    0: \"blue\",\n",
        "    1: \"red\"\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 6*len(columns_to_plot)))\n",
        "fig.subplots_adjust(\n",
        "    wspace=0.4,\n",
        "    hspace=0.7\n",
        ")\n",
        "\n",
        "axes_flat = axes.flatten()\n",
        "for i, column in enumerate(columns_to_plot):\n",
        "    for i_add, split in enumerate([\"train\", \"val\"]):\n",
        "      ax = axes_flat[i*2+i_add]\n",
        "      # norm_val = df.loc[df[\"split\"] == split, column].value_counts().sum()\n",
        "      norm_val = 1\n",
        "      ax_inner = (df.loc[df[\"split\"] == split, column].value_counts()/norm_val).plot.bar(\n",
        "          title = column + \" \" + split,\n",
        "          ax=ax, color=colors[i_add],\n",
        "          alpha=0.2,\n",
        "          xlabel=\"\"\n",
        "      )\n",
        "      ax_inner.set_xticklabels(ax_inner.get_xticklabels(), rotation=45, ha='right')\n",
        "      ax.legend([split])"
      ],
      "metadata": {
        "id": "vHXi6xTVfWsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in [\"train\", \"val\"]:\n",
        "\n",
        "  data = df.loc[df[\"split\"] == split, \"text_len\"].copy()\n",
        "  data = data.clip(lower=data.quantile(0.005), upper=data.quantile(0.995))\n",
        "  ax = plt.hist(data, density=True, bins = 50)\n",
        "  plt.xlabel(\"длина текста\")\n",
        "  plt.title(f\"длина текста {split}\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "apintJLjftnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df[\"split\"] == split, \"text_len\"].quantile(0.98)"
      ],
      "metadata": {
        "id": "74eZVjpJf2R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы из визуализации данных:**\n",
        "* Не очень понятен принцип разделения на train/val;\n",
        "* Есть небольшой дисбаланс классов, если будут \"микро\" метрики, то важно учесть;\n",
        "* Распределение по типам источников данных в train/val +- совпадает. В test может отличаться;\n",
        "* Оснвная масса текстов длиной до 7к символов ~= 2.5к токенов;\n",
        "\n"
      ],
      "metadata": {
        "id": "jtYj1m_df7Kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2\n",
        "## Цель задачи - многоклассовая классификация данных (подкатегории сгенерированных и частично сгенерированных данных + полностью написанные человеком)\n",
        "## Список классов:\n",
        "* Fully human-written: The document is entirely authored by a human without any AI assistance.\n",
        "* Human-initiated, then machine-continued: A human starts writing, and an AI model completes the text.\n",
        "* Human-written, then machine-polished: The text is initially written by a human but later refined or edited by an AI model.\n",
        "* Machine-written, then machine-humanized (obfuscated): An AI generates the text, which is later modified to obscure its machine origin.\n",
        "* Machine-written, then human-edited: The content is generated by an AI but subsequently edited or refined by a human.\n",
        "* Deeply-mixed text: The document contains interwoven sections written by both humans and AI, without a clear separation."
      ],
      "metadata": {
        "id": "1V6udI__-eTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Загрузка данных"
      ],
      "metadata": {
        "id": "E7V2qV0A_t3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1rNQTkhkVG9nzcT97Nk_WyJd80ZaacT0- # dev file\n",
        "!gdown 1u5C4o_fmjL5nQ_RtgLDShuG97Ix6_KGK # train file"
      ],
      "metadata": {
        "id": "vVbD7-Xs_qWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.read_json(\"subtask2_dev.jsonl\", lines=True)\n",
        "train_df = pd.read_json(\"subtask2_train.jsonl\", lines=True)"
      ],
      "metadata": {
        "id": "1nrCqERn_7Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Размер валидации: {val_df.shape[0]}\")\n",
        "val_df.head()"
      ],
      "metadata": {
        "id": "azwxTSAeBLa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Размер трейна: {train_df.shape[0]}\")\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "2Q6xfUJ5Bom0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df[\"split\"] = \"val\"\n",
        "train_df[\"split\"] = \"train\"\n",
        "df = pd.concat([train_df, val_df])"
      ],
      "metadata": {
        "id": "TDTxi5aeFdfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "jQlh_D36F4pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum() # Данные без пропусков"
      ],
      "metadata": {
        "id": "AYwYYLA4HE1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "посмотрим на распределение по моделям, датасетам и таргетам"
      ],
      "metadata": {
        "id": "PwpUwWrtEAUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text_len\"] = df[\"text\"].str.len()"
      ],
      "metadata": {
        "id": "fogBh2HREXo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_plot = ['language', 'source_dataset', 'model', 'label_text']\n",
        "colors = {\n",
        "    0: \"blue\",\n",
        "    1: \"red\"\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12, 6*len(columns_to_plot)))\n",
        "fig.subplots_adjust(\n",
        "    wspace=0.4,\n",
        "    hspace=0.7\n",
        ")\n",
        "\n",
        "axes_flat = axes.flatten()\n",
        "for i, column in enumerate(columns_to_plot):\n",
        "    for i_add, split in enumerate([\"train\", \"val\"]):\n",
        "      ax = axes_flat[i*2+i_add]\n",
        "      # norm_val = df.loc[df[\"split\"] == split, column].value_counts().sum()\n",
        "      norm_val = 1\n",
        "      ax_inner = (df.loc[df[\"split\"] == split, column].value_counts()/norm_val).plot.bar(\n",
        "          title = column + \" \" + split,\n",
        "          ax=ax, color=colors[i_add],\n",
        "          alpha=0.2,\n",
        "          xlabel=\"\"\n",
        "      )\n",
        "      ax_inner.set_xticklabels(ax_inner.get_xticklabels(), rotation=45, ha='right')\n",
        "      ax.legend([split])"
      ],
      "metadata": {
        "id": "xq2XL1TPMhwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in [\"train\", \"val\"]:\n",
        "\n",
        "  data = df.loc[df[\"split\"] == split, \"text_len\"].copy()\n",
        "  data = data.clip(lower=data.quantile(0.005), upper=data.quantile(0.995))\n",
        "  ax = plt.hist(data, density=True, bins = 50)\n",
        "  plt.xlabel(\"длина текста\")\n",
        "  plt.title(f\"длина текста {split}\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qi9W7xc-ZB_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df[\"split\"] == split, \"text_len\"].quantile(0.98)"
      ],
      "metadata": {
        "id": "A8xtbo5hepVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы из визуализации данных:**\n",
        "* Не очень понятен принцип разделения на train/val;\n",
        "* Данные только на английском (хотя в задании указана мультиязычность);\n",
        "* Есть сильный дисбаланс классов, если будут \"микро\" метрики, то важно учесть;\n",
        "* Распределение по датасетам данных в train/val сильно отличается. В test скорее всего будет вообще другой набор исходных датасетов, тк они в открыом доступе (но можно проверить на такого рода лики :) );\n",
        "* Сложно восстанвливать баланс, скорее всего наиболее подходящие для обучения будут metric-learning подходы;\n",
        "* Оснвная масса текстов длиной до 8к символов ~= 3к токенов;\n",
        "* Ссылки на статьи по используемым данным:\n",
        "  - [LLM-DetectAlive](https://arxiv.org/abs/2408.04284);\n",
        "  - [m4gt-bench](https://arxiv.org/abs/2402.11175) тут прям есть [данные](https://drive.google.com/drive/folders/1hBgW6sgZfz1BK0lVdUu0bZ4HPKSpOMSY);\n",
        "  - [RoFT](https://arxiv.org/abs/2010.03070)\n",
        "  - [MixSet](MixSet);\n",
        "  - [TriBERT](https://arxiv.org/abs/2110.13412);\n",
        "  - [RoFT_chatgpt](https://paperswithcode.com/dataset/roft-chatgpt) тут, кстати, гигачек тестили;\n",
        "  - [Coauthor](https://arxiv.org/abs/2201.06796);\n",
        "  - [LAMP](https://arxiv.org/abs/2304.11406);\n",
        "* Ссылки на допонительные датасеты по тематике:\n",
        "  - [датасеты с hf по детекции генеративного текста](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=trending&search=generat) тут в основном отзывы;\n",
        "\n"
      ],
      "metadata": {
        "id": "uxJmjomNdL29"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmhzkHRujZWJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}