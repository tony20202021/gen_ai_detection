# Проект обнаружения генеративного ИИ (Generative AI Detection)

## Обзор проекта

Данный проект предоставляет инструменты для обнаружения текстов, созданных или модифицированных с помощью генеративного искусственного интеллекта. Проект основан на задачах из соревнования PAN 2025 по обнаружению генеративного ИИ в текстах.

## Особенности проекта

- **Две задачи классификации**:
  - **Бинарная классификация**: Определение, является ли текст написанным человеком или сгенерированным ИИ
  - **Многоклассовая классификация**: Определение конкретного типа текста (полностью человеческий, смешанный, различные формы генерации ИИ)

- **Комплексный анализ**:
  - Извлечение эмбедингов текстов с использованием предобученных языковых моделей
  - Понижение размерности для визуализации распределения текстов
  - Кластеризация для выявления естественных групп текстов
  - Обучение различных моделей классификации

- **Визуализация и интерпретация**:
  - Интерактивные графики для анализа пространства эмбедингов
  - Оценка соответствия кластеров истинным меткам
  - Анализ сложных для классификации примеров

- **Генерация файлов для отправки**:
  - Автоматическая генерация предсказаний в формате, требуемом соревнованием PAN 2025
  - Поддержка различных форматов классов для обеих задач

## Установка и настройка

### Требования

- Python 3.8+
- Conda (рекомендуется для управления окружением)
- GPU с поддержкой CUDA (опционально, но рекомендуется для ускорения)

### Настройка окружения

1. Клонировать репозиторий:
   ```bash
   git clone https://github.com/yourusername/genai-detection.git
   cd genai-detection
   ```

2. Создать и активировать окружение:
   ```bash
   # Для Linux/macOS
   conda env create -f environment.yml
   conda activate amikhalev_gen_ai_detection
   ```

### Скачивание данных

Запустите скрипты для скачивания данных:
```bash
# Для задачи 1 (бинарная классификация)
python gen_ai_detection/scripts/download_task1_data.py

# Для задачи 2 (многоклассовая классификация)
python gen_ai_detection/scripts/download_task2_data.py

# Или скачайте все данные сразу
bash gen_ai_detection/scripts/download_all_data.sh
```

## Использование

### Запуск полного пайплайна

Для запуска полного пайплайна анализа выполните:

```bash
# Для Linux/macOS
bash ./scripts/run_pipeline.sh -t 2 -m sentence-transformers/all-mpnet-base-v2
```

Параметры запуска:
- `-t, --task`: Номер задачи (1 - бинарная, 2 - многоклассовая)
- `-m, --model`: Модель для извлечения эмбедингов
- `-c, --clustering`: Метод кластеризации (kmeans, dbscan, agglomerative)
- `--model-type`: Тип модели классификации (logistic, svm, rf, xgb, mlp, all)
- `--grid-search`: Включить поиск гиперпараметров
- `--no-pca`: Не использовать PCA для понижения размерности
- `--no-balance`: Не учитывать несбалансированность классов

### Запуск отдельных этапов

#### 1. Извлечение эмбедингов

```bash
python gen_ai_detection/src/extract_embeddings.py --task 1 --model sentence-transformers/all-mpnet-base-v2 --reduce_dim
```

#### 2. Кластеризация

```bash
python gen_ai_detection/src/clustering.py --task 1 --embeddings_path gen_ai_detection/results/embeddings/task1_embeddings_all-mpnet-base-v2_with_dim_reduction.pkl --method kmeans --use_pca
```

#### 3. Обучение моделей

```bash
python gen_ai_detection/src/train_models.py --task 1 --embeddings_path gen_ai_detection/results/embeddings/task1_embeddings_all-mpnet-base-v2_with_dim_reduction.pkl --model_type all --balance
```

#### 4. Генерация файлов для отправки

```bash
python gen_ai_detection/src/generate_submissions.py --task 1 --model_path gen_ai_detection/results/models/task1_xgb_model_20250516_120000.joblib --test_data_path path/to/test_data.jsonl --version v01
```

#### 5. Визуализация результатов

```bash
jupyter notebook gen_ai_detection/notebooks/visualize_embeddings.ipynb
```

## Структура проекта

```
gen_ai_detection/
├── data/                              # Директория с данными
│   ├── task1/                         # Данные для бинарной классификации
│   └── task2/                         # Данные для многоклассовой классификации
├── gen_ai_detection/                  # Основной код проекта
│   ├── logs/                          # Логи выполнения скриптов
│   ├── notebooks/                     # Jupyter ноутбуки для анализа
│   │   └── visualize_embeddings.ipynb # Ноутбук для визуализации результатов
│   ├── results/                       # Результаты работы скриптов
│   │   ├── clustering/                # Результаты кластеризации
│   │   ├── embeddings/                # Извлеченные эмбединги
│   │   ├── encoders/                  # Сохраненные LabelEncoder для задач
│   │   ├── models/                    # Обученные модели
│   │   └── submissions/               # Сгенерированные файлы для отправки
│   ├── scripts/                       # Вспомогательные скрипты
│   │   ├── download_all_data.sh       # Скрипт для скачивания всех данных
│   │   ├── download_task1_data.py     # Скрипт для скачивания данных задачи 1
│   │   ├── download_task2_data.py     # Скрипт для скачивания данных задачи 2
│   │   ├── run_pipeline.sh            # Скрипт для запуска полного пайплайна
│   │   └── setup_environment.sh       # Скрипт для настройки окружения
│   └── src/                           # Исходный код проекта
│       ├── clustering.py              # Скрипт для кластеризации
│       ├── extract_embeddings.py      # Скрипт для извлечения эмбедингов
│       ├── generate_submissions.py    # Скрипт для генерации предсказаний
│       ├── train_models.py            # Скрипт для обучения моделей
│       └── utils_visualization.py     # Модуль визуализации
├── environment.yml                    # Конфигурация окружения conda
├── requirements.txt                   # Зависимости Python
└── README.md                          # Документация проекта
```

## Технический подход

### 1. Получение эмбедингов текста

Для извлечения эмбедингов используются предобученные языковые модели из библиотеки Hugging Face Transformers. По умолчанию используется модель `all-mpnet-base-v2`, которая хорошо работает для задач кластеризации и классификации текстов.

Извлеченные эмбединги сохраняются в формате Pickle и могут использоваться для дальнейшего анализа.

### 2. Понижение размерности и визуализация

Для визуализации высокоразмерных эмбедингов применяются методы понижения размерности:
- **PCA**: Для начального снижения размерности
- **t-SNE**: Для дальнейшей визуализации в 2D-пространстве

Визуализация помогает понять распределение текстов в пространстве эмбедингов и оценить потенциальную разделимость классов.

### 3. Кластеризация

Реализовано несколько методов кластеризации:
- **K-means**: Классический алгоритм, хорошо работает, когда кластеры имеют сферическую форму
- **DBSCAN**: Плотностной алгоритм, полезный для обнаружения кластеров произвольной формы
- **Agglomerative Clustering**: Иерархический подход к кластеризации

Результаты кластеризации сравниваются с истинными метками для оценки естественной разделимости классов.

### 4. Обучение моделей классификации

Проект поддерживает несколько алгоритмов машинного обучения:
- **Логистическая регрессия**: Простая и интерпретируемая модель
- **SVM**: Эффективный для задач с высокоразмерными признаками
- **Random Forest**: Робастный ансамблевый метод
- **XGBoost**: Современный алгоритм градиентного бустинга
- **MLP**: Многослойный персептрон (нейронная сеть)

Модели обучаются с учетом несбалансированности классов и оцениваются с использованием кросс-валидации.

### 5. Генерация предсказаний для отправки

Реализован механизм генерации предсказаний для новых данных с использованием обученных моделей:
- Автоматическое извлечение эмбедингов из тестовых текстов
- Применение обученной модели для получения предсказаний
- Корректная обработка меток для многоклассовой задачи (отображение в числовые идентификаторы 0-5)
- Сохранение предсказаний в требуемом JSONL формате для отправки

## Особенности данных и рекомендации

- **Несбалансированность классов**: Особенно заметна в задаче многоклассовой классификации. Рекомендуется использовать взвешенные метрики и методы балансировки.
  
- **Различия в распределении**: Тренировочные и валидационные данные могут иметь разные распределения. Рекомендуется использовать подходы metric learning для лучшей генерализации.
  
- **Оптимальная длина текста**: Большинство текстов имеют длину до 7-8 тысяч символов (~2.5-3 тысячи токенов). Учитывайте это при настройке параметра `max_length`.

## Рекомендации по улучшению

1. **Дополнительные признаки**: Помимо эмбедингов, можно использовать стилистические и лингвистические признаки (частота редких слов, соотношение частей речи и т.д.).

2. **Ансамблирование моделей**: Комбинирование нескольких моделей может повысить устойчивость классификации.

3. **Аугментация данных**: Для недопредставленных классов можно применять техники аугментации текстов.

4. **Предобучение на дополнительных данных**: Использование моделей, предобученных на задачах обнаружения сгенерированных текстов.

5. **Пост-обработка предсказаний**: Калибровка вероятностей и использование различных порогов для повышения точности на отдельных классах.

## Вклад в проект

Пожалуйста, не стесняйтесь вносить свой вклад в проект! Вы можете помочь:
- Исправляя ошибки
- Оптимизируя код
- Добавляя новые модели и методы
- Улучшая документацию
- Предлагая новые идеи и подходы

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности смотрите в файле LICENSE.

## Благодарности

- PAN 2025 за предоставление данных и постановку задачи
- Авторам оригинальных датасетов, использованных в соревновании
- Сообществу Hugging Face за предоставление предобученных моделей