# Проект обнаружения генеративного ИИ (Generative AI Detection)

## Обзор проекта

Данный проект предоставляет инструменты для обнаружения текстов, созданных или модифицированных с помощью генеративного искусственного интеллекта. Проект основан на задачах из соревнования PAN 2025 по обнаружению генеративного ИИ в текстах.

## Особенности проекта

- **Две задачи классификации**:
  - **Бинарная классификация**: Определение, является ли текст написанным человеком или сгенерированным ИИ
  - **Многоклассовая классификация**: Определение конкретного типа текста (полностью человеческий, смешанный, различные формы генерации ИИ)

- **Комплексный анализ**:
  - Извлечение эмбедингов текстов с использованием предобученных языковых моделей
  - Понижение размерности для визуализации распределения текстов
  - Кластеризация для выявления естественных групп текстов
  - Обучение различных моделей классификации

- **Визуализация и интерпретация**:
  - Интерактивные графики для анализа пространства эмбедингов
  - Оценка соответствия кластеров истинным меткам
  - Анализ сложных для классификации примеров

## Установка и настройка

### Требования

- Python 3.8+
- Conda (рекомендуется для управления окружением)
- GPU с поддержкой CUDA (опционально, но рекомендуется для ускорения)

### Настройка окружения

1. Клонировать репозиторий:
   ```bash
   git clone https://github.com/yourusername/genai-detection.git
   cd genai-detection
   ```

2. Создать и активировать окружение:
   ```bash
   # Для Linux/macOS
   bash setup_environment.sh
   
   # Для Windows
   setup_environment.bat
   ```

### Скачивание данных

Запустите скрипты для скачивания данных:
```bash
# Для задачи 1 (бинарная классификация)
python download_task1_data.py

# Для задачи 2 (многоклассовая классификация)
python download_task2_data.py

# Или скачайте все данные сразу
bash download_all_data.sh
```

## Использование

### Запуск полного пайплайна

Для запуска полного пайплайна анализа выполните:

```bash
# Для Linux/macOS
bash run_pipeline.sh -t 1 -m sentence-transformers/all-mpnet-base-v2

# Для Windows
run_pipeline.bat -t 1 -m sentence-transformers/all-mpnet-base-v2
```

Параметры запуска:
- `-t, --task`: Номер задачи (1 - бинарная, 2 - многоклассовая)
- `-m, --model`: Модель для извлечения эмбедингов
- `-c, --clustering`: Метод кластеризации (kmeans, dbscan, agglomerative)
- `--model-type`: Тип модели классификации (logistic, svm, rf, xgb, mlp, all)
- `--grid-search`: Включить поиск гиперпараметров
- `--no-pca`: Не использовать PCA для понижения размерности
- `--no-balance`: Не учитывать несбалансированность классов

### Запуск отдельных этапов

#### 1. Извлечение эмбедингов

```bash
python extract_embeddings.py --task 1 --model sentence-transformers/all-mpnet-base-v2 --reduce_dim
```

#### 2. Кластеризация

```bash
python clustering.py --task 1 --embeddings_path data/embeddings/task1_embeddings_all-mpnet-base-v2_with_dim_reduction.pkl --method kmeans --use_pca
```

#### 3. Обучение моделей

```bash
python train_models.py --task 1 --embeddings_path data/embeddings/task1_embeddings_all-mpnet-base-v2_with_dim_reduction.pkl --model_type all --balance
```

#### 4. Визуализация результатов

```bash
jupyter notebook visualize_embeddings.ipynb
```

## Структура проекта

```
.
├── data/                              # Директория с данными
│   ├── task1/                         # Данные для бинарной классификации
│   ├── task2/                         # Данные для многоклассовой классификации
│   ├── embeddings/                    # Извлеченные эмбединги
│   └── clustering/                    # Результаты кластеризации
├── models/                            # Обученные модели
├── results/                           # Результаты и метрики
├── download_task1_data.py             # Скрипт для скачивания данных (задача 1)
├── download_task2_data.py             # Скрипт для скачивания данных (задача 2)
├── download_all_data.sh               # Скрипт для скачивания всех данных
├── extract_embeddings.py              # Скрипт для извлечения эмбедингов
├── clustering.py                      # Скрипт для кластеризации
├── train_models.py                    # Скрипт для обучения моделей
├── visualize_embeddings.ipynb         # Ноутбук для визуализации результатов
├── run_pipeline.sh                    # Скрипт для запуска полного пайплайна (Linux/macOS)
├── run_pipeline.bat                   # Скрипт для запуска полного пайплайна (Windows)
├── setup_environment.sh               # Скрипт для настройки окружения (Linux/macOS)
├── setup_environment.bat              # Скрипт для настройки окружения (Windows)
├── environment.yml                    # Конфигурация окружения conda
├── requirements.txt                   # Зависимости Python
└── README.md                          # Документация проекта
```

## Технический подход

### 1. Получение эмбедингов текста

Для извлечения эмбедингов используются предобученные языковые модели из библиотеки Hugging Face Transformers. По умолчанию используется модель `all-mpnet-base-v2`, которая хорошо работает для задач кластеризации и классификации текстов.

Извлеченные эмбединги сохраняются в формате Pickle и могут использоваться для дальнейшего анализа.

### 2. Понижение размерности и визуализация

Для визуализации высокоразмерных эмбедингов применяются методы понижения размерности:
- **PCA**: Для начального снижения размерности
- **t-SNE**: Для дальнейшей визуализации в 2D-пространстве

Визуализация помогает понять распределение текстов в пространстве эмбедингов и оценить потенциальную разделимость классов.

### 3. Кластеризация

Реализовано несколько методов кластеризации:
- **K-means**: Классический алгоритм, хорошо работает, когда кластеры имеют сферическую форму
- **DBSCAN**: Плотностной алгоритм, полезный для обнаружения кластеров произвольной формы
- **Agglomerative Clustering**: Иерархический подход к кластеризации

Результаты кластеризации сравниваются с истинными метками для оценки естественной разделимости классов.

### 4. Обучение моделей классификации

Проект поддерживает несколько алгоритмов машинного обучения:
- **Логистическая регрессия**: Простая и интерпретируемая модель
- **SVM**: Эффективный для задач с высокоразмерными признаками
- **Random Forest**: Робастный ансамблевый метод
- **XGBoost**: Современный алгоритм градиентного бустинга
- **MLP**: Многослойный персептрон (нейронная сеть)

Модели обучаются с учетом несбалансированности классов и оцениваются с использованием кросс-валидации.

## Особенности данных и рекомендации

- **Несбалансированность классов**: Особенно заметна в задаче многоклассовой классификации. Рекомендуется использовать взвешенные метрики и методы балансировки.
  
- **Различия в распределении**: Тренировочные и валидационные данные могут иметь разные распределения. Рекомендуется использовать подходы metric learning для лучшей генерализации.
  
- **Оптимальная длина текста**: Большинство текстов имеют длину до 7-8 тысяч символов (~2.5-3 тысячи токенов). Учитывайте это при настройке параметра `max_length`.

## Рекомендации по улучшению

1. **Дополнительные признаки**: Помимо эмбедингов, можно использовать стилистические и лингвистические признаки (частота редких слов, соотношение частей речи и т.д.).

2. **Ансамблирование моделей**: Комбинирование нескольких моделей может повысить устойчивость классификации.

3. **Аугментация данных**: Для недопредставленных классов можно применять техники аугментации текстов.

4. **Предобучение на дополнительных данных**: Использование моделей, предобученных на задачах обнаружения сгенерированных текстов.

## Вклад в проект

Пожалуйста, не стесняйтесь вносить свой вклад в проект! Вы можете помочь:
- Исправляя ошибки
- Оптимизируя код
- Добавляя новые модели и методы
- Улучшая документацию
- Предлагая новые идеи и подходы

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности смотрите в файле LICENSE.

## Благодарности

- PAN 2025 за предоставление данных и постановку задачи
- Авторам оригинальных датасетов, использованных в соревновании
- Сообществу Hugging Face за предоставление предобученных моделей